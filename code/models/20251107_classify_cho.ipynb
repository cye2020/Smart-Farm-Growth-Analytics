{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28021e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ \n",
    "\n",
    "# 2-1. ì‹œê°í™”\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "# 2-2. \n",
    "import shap\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, make_scorer, recall_score, \n",
    "    precision_score, f1_score, fbeta_score, average_precision_score, balanced_accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "from scipy.stats import uniform, randint\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pkg_resources\")\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils import DATA_DIR, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e26762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "if platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "elif platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "else:  # Linux\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# í°íŠ¸ ê°œì¸ ê²½ë¡œì— ë§ì¶°ì„œ ë³€ê²½\n",
    "# FONT_DIR = Path(\"/path/to/fonts\")\n",
    "# font_path = FONT_DIR / 'FREESENTATION-6SEMIBOLD.ttf'\n",
    "# prop = fm.FontProperties(fname=font_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced663af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_kwargs = {\n",
    "    'parse_dates': ['ê²€ì •ì¼ì'],\n",
    "    'date_format': '%Y-%m-%d'\n",
    "}\n",
    "\n",
    "milk: pd.DataFrame = pd.read_csv(DATA_DIR /'interim' / 'milk.csv', **pandas_kwargs)\n",
    "milk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = milk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551af10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1ë“±ê¸‰'] = df['ìš°ìœ ë“±ê¸‰'].map({\n",
    "    '1ë“±ê¸‰': 0,\n",
    "    '2ë“±ê¸‰ì´í•˜': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target='1ë“±ê¸‰'\n",
    "features = [\n",
    "    'ëˆ„ì ì°©ìœ ì¼(ì—°ê³„)',\n",
    "    'ì „ì‚°ì°¨ë¹„ìœ ì§€ì†ì„±',\n",
    "    'í˜„ì¬ì‚°ì°¨ë¹„ìœ ì§€ì†ì„±',\n",
    "    'ë¹„ìœ ìµœê³ ë„ë‹¬ì¼ìˆ˜_log',\n",
    "    'ê±´ìœ ì „ë§ˆì§€ë§‰ìœ ëŸ‰_log',\n",
    "    'ì „ì‚°ì°¨ê±´ìœ ì „ìœ ëŸ‰',\n",
    "    'ì‚°ì°¨',\n",
    "    'ë†í›„ì‚¬ë£Œê¸‰ì—¬ëŸ‰(ì—°ê³„)',\n",
    "    'ê³µíƒœì¼ìˆ˜_log'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['ê²€ì •ì¼ì'].dt.year == 2020]\n",
    "test = df[df['ê²€ì •ì¼ì'].dt.year == 2021]\n",
    "\n",
    "X_train, X_test = train[features], test[features]\n",
    "y_train, y_test = train['1ë“±ê¸‰'], test['1ë“±ê¸‰']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_pr_auc_scorer(estimator, X, y_true):\n",
    "    try:\n",
    "        y_pred_proba = estimator.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # NaN ì²´í¬\n",
    "        if np.isnan(y_pred_proba).any():\n",
    "            return 0.0\n",
    "        \n",
    "        # ì†Œìˆ˜ í´ë˜ìŠ¤ ì¡´ì¬ í™•ì¸\n",
    "        if y_true.sum() == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return average_precision_score(y_true, y_pred_proba)\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(\"í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\në¹„ìœ¨:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "\n",
    "# âœ… ìˆ˜ì •: 0ì´ 1ë“±ê¸‰(ë‹¤ìˆ˜), 1ì´ ê·¸ì™¸(ì†Œìˆ˜)\n",
    "majority_count = (y_train == 0).sum()   # 1ë“±ê¸‰ (ë‹¤ìˆ˜) - ì•½ 90%\n",
    "minority_count = (y_train == 1).sum()   # ê·¸ì™¸ (ì†Œìˆ˜) - ì•½ 10%\n",
    "\n",
    "print(f\"\\ní´ë˜ìŠ¤ êµ¬ì„±:\")\n",
    "print(f\"   0 (1ë“±ê¸‰, ë‹¤ìˆ˜): {majority_count}ê°œ ({majority_count/len(y_train)*100:.1f}%)\")\n",
    "print(f\"   1 (ê·¸ì™¸, ì†Œìˆ˜): {minority_count}ê°œ ({minority_count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# âœ… scale_pos_weight: ì–‘ì„± í´ë˜ìŠ¤(1=ê·¸ì™¸=ì†Œìˆ˜)ì— ëŒ€í•œ ê°€ì¤‘ì¹˜\n",
    "# ê³µì‹: (ë‹¤ìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜) / (ì†Œìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜)\n",
    "scale_pos_weight = majority_count / minority_count\n",
    "\n",
    "print(f\"\\nê³„ì‚°ëœ scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "print(f\"   â†’ LightGBMì€ ìë™ìœ¼ë¡œ í´ë˜ìŠ¤ 1(ê·¸ì™¸)ì— ì´ ê°€ì¤‘ì¹˜ ì ìš©\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "print(\"\\nê²°ì¸¡ì¹˜ í˜„í™©:\")\n",
    "missing_info = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'ê²°ì¸¡_ê°œìˆ˜': X_train.isnull().sum().values,\n",
    "    'ê²°ì¸¡_ë¹„ìœ¨(%)': (X_train.isnull().mean() * 100).values\n",
    "}).sort_values('ê²°ì¸¡_ë¹„ìœ¨(%)', ascending=False)\n",
    "print(missing_info.head(10))\n",
    "print(f\"\\nì „ì²´ ê²°ì¸¡ ë¹„ìœ¨: {X_train.isnull().mean().mean()*100:.1f}%\")\n",
    "print(f\"Feature ê°œìˆ˜: {X_train.shape[1]}ê°œ\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Pipeline êµ¬ì„±\n",
    "# ============================================\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('model', LGBMClassifier(\n",
    "        objective='binary',\n",
    "        device='cpu',\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=scale_pos_weight,  # âœ… ìë™ìœ¼ë¡œ pos_label=1ì— ì ìš©\n",
    "        force_col_wise=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\nâœ… Pipeline êµ¬ì„±:\")\n",
    "print(\"   1. LGBMClassifier (GPU)\")\n",
    "print(f\"      - scale_pos_weight={scale_pos_weight:.2f} (í´ë˜ìŠ¤ 1=ê·¸ì™¸ ê°€ì¤‘ì¹˜)\")\n",
    "\n",
    "# ============================================\n",
    "# 3. íŒŒë¼ë¯¸í„° ë¶„í¬\n",
    "# ============================================\n",
    "print(f\"\\nì†Œìˆ˜ í´ë˜ìŠ¤(1=ê·¸ì™¸) ìƒ˜í”Œ ìˆ˜: {minority_count}ê°œ\")\n",
    "\n",
    "param_distributions = {\n",
    "    # ğŸ”¥ LightGBM íŒŒë¼ë¯¸í„° - ì†Œìˆ˜ í´ë˜ìŠ¤ ë¯¼ê°ë„ í–¥ìƒ\n",
    "    'model__n_estimators': randint(300, 1000),\n",
    "    'model__learning_rate': uniform(0.005, 0.045),\n",
    "    'model__max_depth': randint(3, 8),\n",
    "    'model__num_leaves': randint(15, 80),\n",
    "    'model__min_child_samples': randint(5, 50),\n",
    "    'model__subsample': uniform(0.6, 0.4),\n",
    "    'model__colsample_bytree': uniform(0.6, 0.4),\n",
    "    'model__min_child_weight': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'model__reg_alpha': uniform(0, 0.5),\n",
    "    'model__reg_lambda': uniform(0, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Scoring ì„¤ì •\n",
    "# ============================================\n",
    "\n",
    "# âœ… ì´ì œ pos_label=1ì´ ê¸°ë³¸ê°’ì´ë¯€ë¡œ ìƒëµ ê°€ëŠ¥!\n",
    "# í•˜ì§€ë§Œ ëª…í™•ì„±ì„ ìœ„í•´ ëª…ì‹œí•˜ëŠ” ê²ƒ ì¶”ì²œ (íŠ¹íˆ íŒ€ í˜‘ì—… ì‹œ)\n",
    "\n",
    "# def constrained_pr_auc_scorer(y_true, y_pred_proba):\n",
    "#     \"\"\"\n",
    "#     Recall >= 0.5 ì œì•½ í•˜ì—ì„œ PR-AUC ìµœëŒ€í™”\n",
    "    \n",
    "#     ë¡œì§:\n",
    "#     1. í˜„ì¬ í™•ë¥ ë¡œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
    "#     2. Recall >= 0.5 ë§Œì¡±í•˜ëŠ” ì„ê³„ê°’ ì¤‘ ìµœëŒ€ Precision ì°¾ê¸°\n",
    "#     3. ê·¸ë•Œì˜ PR-AUC ë°˜í™˜\n",
    "#     4. ë§Œì¡±í•˜ëŠ” ì„ê³„ê°’ ì—†ìœ¼ë©´ ë§¤ìš° ë‚®ì€ ì ìˆ˜ ë°˜í™˜\n",
    "#     \"\"\"\n",
    "#     from sklearn.metrics import precision_recall_curve, auc\n",
    "    \n",
    "#     # PR ê³¡ì„  ê³„ì‚°\n",
    "#     precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba, pos_label=1)\n",
    "    \n",
    "#     # Recall >= 0.5 ë§Œì¡±í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "#     valid_idx = np.where(recalls >= 0.5)[0]\n",
    "    \n",
    "#     if len(valid_idx) == 0:\n",
    "#         # Recall 0.5 ë‹¬ì„± ë¶ˆê°€ â†’ ë§¤ìš° ë‚®ì€ í˜ë„í‹° ì ìˆ˜\n",
    "#         return -1.0\n",
    "    \n",
    "#     # Recall >= 0.5 êµ¬ê°„ì˜ PR-AUC ê³„ì‚°\n",
    "#     # ë°©ë²• 1: ì „ì²´ PR-AUC ë°˜í™˜ (í•˜ì§€ë§Œ Recall 0.5 ì œì•½ ë°˜ì˜)\n",
    "#     valid_precisions = precisions[valid_idx]\n",
    "#     valid_recalls = recalls[valid_idx]\n",
    "    \n",
    "#     # Recall êµ¬ê°„ [0.5, 1.0]ì—ì„œì˜ AUC\n",
    "#     # ê°„ë‹¨í•˜ê²Œ: Recall >= 0.5ì¸ êµ¬ê°„ì˜ í‰ê·  Precision ì‚¬ìš©\n",
    "#     score = np.mean(valid_precisions)\n",
    "    \n",
    "#     return score\n",
    "\n",
    "\n",
    "scoring = {\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f2': make_scorer(fbeta_score, beta=2),              \n",
    "    'pr_auc': make_scorer(average_precision_score, response_method='predict_proba'),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0, average='macro'),\n",
    "    'roc_auc': 'roc_auc'                                           \n",
    "}\n",
    "\n",
    "main_score = 'pr_auc'\n",
    "\n",
    "print(\"\\nâœ… Scoring ì „ëµ:\")\n",
    "print(f\"   ë©”ì¸ ëª©í‘œ: {main_score.upper()}\")\n",
    "print(f\"   íƒ€ê¹ƒ: í´ë˜ìŠ¤ 1 (ê·¸ì™¸) ê²€ì¶œ ìµœì í™”\")\n",
    "print(f\"   pos_label: 1 (ê¸°ë³¸ê°’, ìƒëµ ê°€ëŠ¥)\")\n",
    "\n",
    "# ============================================\n",
    "# 5. Stratified K-Fold\n",
    "# ============================================\n",
    "n_folds = 5\n",
    "n_iter = 150\n",
    "cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"\\nêµì°¨ê²€ì¦ Folds: {n_folds}\")\n",
    "print(f\"ëœë¤ ìƒ˜í”Œë§ ì¡°í•© ìˆ˜: {n_iter}ê°œ\")\n",
    "print(f\"ì´ Fits: {n_iter} Ã— {n_folds} = {n_iter * n_folds}\")\n",
    "\n",
    "# ============================================\n",
    "# 6. RandomizedSearchCV ì‹¤í–‰\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ RandomizedSearchCV ì‹œì‘!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lgbm_random = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iter,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    refit=main_score,\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    error_score=np.nan \n",
    ")\n",
    "\n",
    "start_actual = time.time()\n",
    "lgbm_random.fit(X_train, y_train)\n",
    "actual_time = time.time() - start_actual\n",
    "\n",
    "# ============================================\n",
    "# 7. ê²°ê³¼ ì¶œë ¥\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"â±ï¸  ì†Œìš” ì‹œê°„: {actual_time/60:.1f}ë¶„ ({actual_time:.0f}ì´ˆ)\")\n",
    "\n",
    "print(f\"\\nğŸ† ìµœê³  {main_score.upper()} (CV): {lgbm_random.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ ìµœì  íŒŒë¼ë¯¸í„°:\")\n",
    "for key, value in sorted(lgbm_random.best_params_.items()):\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# 8. ì „ì²´ ê²°ê³¼ ë¹„êµ\n",
    "# ============================================\n",
    "results_df = pd.DataFrame(lgbm_random.cv_results_)\n",
    "\n",
    "scoring_cols = [f'mean_test_{score}' for score in scoring.keys()]\n",
    "\n",
    "comparison_cols = [f'rank_test_{main_score}'] + scoring_cols + ['params']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“Š ìƒìœ„ 10ê°œ íŒŒë¼ë¯¸í„° ì¡°í•© ({main_score.upper()} ê¸°ì¤€)\")\n",
    "print(\"=\"*60)\n",
    "top_10 = results_df[comparison_cols].sort_values(f'rank_test_{main_score}').head(10)\n",
    "\n",
    "for idx, row in top_10.iterrows():\n",
    "    rank = int(row[f'rank_test_{main_score}'])\n",
    "    pr_auc = row[f'mean_test_pr_auc']\n",
    "    recall = row['mean_test_recall']\n",
    "    f2 = row['mean_test_f2']\n",
    "    precision = row['mean_test_precision']\n",
    "    f1 = row['mean_test_f1']\n",
    "    \n",
    "    print(f\"\\n#{rank} - {main_score.upper()}: {row[f'mean_test_{main_score}']:.4f} | Recall: {recall:.4f}\")\n",
    "    print(f\"   F2: {f2:.3f} | Precision: {precision:.3f} | F1: {f1:.3f}\")\n",
    "    \n",
    "    params = row['params']\n",
    "    print(f\"   Model: lr={params.get('model__learning_rate', 0):.4f}, \"\n",
    "          f\"depth={params.get('model__max_depth', 0)}, \"\n",
    "          f\"leaves={params.get('model__num_leaves', 0)}\")\n",
    "\n",
    "# ============================================\n",
    "# 9. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = lgbm_random.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# âœ… í´ë˜ìŠ¤ 1(ê·¸ì™¸)ì˜ í™•ë¥  ì¶”ì¶œ\n",
    "y_pred_proba_minority = y_pred_proba[:, 1]  # âœ… ì¸ë±ìŠ¤ 1 = í´ë˜ìŠ¤ 1 = ê·¸ì™¸\n",
    "\n",
    "print(classification_report(\n",
    "    y_test, y_pred, \n",
    "    target_names=['0 (1ë“±ê¸‰, ë‹¤ìˆ˜)', '1 (ê·¸ì™¸, ì†Œìˆ˜)']\n",
    "))\n",
    "\n",
    "print(\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['ì‹¤ì œ_0 (1ë“±ê¸‰)', 'ì‹¤ì œ_1 (ê·¸ì™¸)'],\n",
    "                     columns=['ì˜ˆì¸¡_0 (1ë“±ê¸‰)', 'ì˜ˆì¸¡_1 (ê·¸ì™¸)'])\n",
    "print(cm_df)\n",
    "\n",
    "# âœ… í˜¼ë™ í–‰ë ¬ í•´ì„ ìˆ˜ì •\n",
    "# confusion_matrix êµ¬ì¡°:\n",
    "#              ì˜ˆì¸¡_0  ì˜ˆì¸¡_1\n",
    "# ì‹¤ì œ_0 (1ë“±ê¸‰)  TN      FP\n",
    "# ì‹¤ì œ_1 (ê·¸ì™¸)   FN      TP\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„ (ì†Œìˆ˜ í´ë˜ìŠ¤=1=ê·¸ì™¸ ê¸°ì¤€):\")\n",
    "print(f\"   âœ… ì •í™•í•œ ë¶„ë¥˜: {tn + tp}ê°œ ({(tn+tp)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"   âš ï¸  ì˜¤íƒ (FP): {fp}ê°œ â†’ 1ë“±ê¸‰ì„ ê·¸ì™¸ë¡œ ì˜¤íŒ (ì¬ê²€ì‚¬ í•„ìš”)\")\n",
    "print(f\"   ğŸš¨ ë¯¸íƒ (FN): {fn}ê°œ â†’ ê·¸ì™¸ë¥¼ 1ë“±ê¸‰ìœ¼ë¡œ ì˜¤íŒ (ì¹˜ëª…ì !)\")\n",
    "\n",
    "# âœ… Recall ê³„ì‚°: TP / (TP + FN)\n",
    "if tp + fn > 0:\n",
    "    recall_pct = tp/(tp+fn)*100\n",
    "    print(f\"\\n   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê·¸ì™¸) Recall: {recall_pct:.1f}%\")\n",
    "    print(f\"      â†’ ê·¸ì™¸ {tp+fn}ê°œ ì¤‘ {tp}ê°œ ê²€ì¶œ ({fn}ê°œ ë†“ì¹¨)\")\n",
    "else:\n",
    "    print(f\"\\n   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê·¸ì™¸) Recall: N/A\")\n",
    "\n",
    "# âœ… Precision ê³„ì‚°: TP / (TP + FP)\n",
    "if tp + fp > 0:\n",
    "    precision_pct = tp/(tp+fp)*100\n",
    "    print(f\"   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê·¸ì™¸) Precision: {precision_pct:.1f}%\")\n",
    "    print(f\"      â†’ ê·¸ì™¸ íŒì • {tp+fp}ê°œ ì¤‘ {tp}ê°œ ì •ë‹µ\")\n",
    "else:\n",
    "    print(f\"   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê·¸ì™¸) Precision: N/A\")\n",
    "\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba_minority)\n",
    "print(f\"   ğŸ“Š ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 10. ğŸ”¥ ì„ê³„ê°’ ì¡°ì •ìœ¼ë¡œ ì¶”ê°€ ê°œì„ \n",
    "# ============================================\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# âœ… pos_label=1 (ê·¸ì™¸) ê¸°ì¤€ìœ¼ë¡œ PR ê³¡ì„  ê³„ì‚°\n",
    "precisions, recalls, thresholds = precision_recall_curve(\n",
    "    y_test, y_pred_proba_minority\n",
    ")\n",
    "\n",
    "# Recall 80% ë³´ì¥\n",
    "target_recall = 0.80\n",
    "idx = np.where(recalls >= target_recall)[0]\n",
    "\n",
    "if len(idx) > 0:\n",
    "    best_idx = idx[np.argmax(precisions[idx])]\n",
    "    optimal_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ¯ Recall {target_recall*100}% ë³´ì¥ ì‹œ ìµœì  ì„ê³„ê°’\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f} (ê¸°ë³¸ê°’ 0.5)\")\n",
    "    print(f\"Precision: {precisions[best_idx]:.3f}\")\n",
    "    print(f\"Recall: {recalls[best_idx]:.3f}\")\n",
    "    \n",
    "    # âœ… ì„ê³„ê°’ ì¡°ì •: í™•ë¥  >= thresholdì´ë©´ í´ë˜ìŠ¤ 1(ê·¸ì™¸)ë¡œ ì˜ˆì¸¡\n",
    "    y_pred_adjusted = (y_pred_proba_minority >= optimal_threshold).astype(int)\n",
    "    \n",
    "    cm_adj = confusion_matrix(y_test, y_pred_adjusted)\n",
    "    tn2, fp2, fn2, tp2 = cm_adj.ravel()\n",
    "    \n",
    "    print(f\"\\nì¡°ì • íš¨ê³¼:\")\n",
    "    print(f\"   ë¯¸íƒ(FN) ê°ì†Œ: {fn} â†’ {fn2} (ê°œì„ : {fn-fn2}ê°œ)\")\n",
    "    print(f\"   ì˜¤íƒ(FP) ì¦ê°€: {fp} â†’ {fp2} (ì¦ê°€: {fp2-fp}ê°œ)\")\n",
    "    \n",
    "    if tp + fn > 0 and tp2 + fn2 > 0:\n",
    "        print(f\"   Recall ê°œì„ : {tp/(tp+fn)*100:.1f}% â†’ {tp2/(tp2+fn2)*100:.1f}%\")\n",
    "    \n",
    "    if fn > fn2:\n",
    "        print(f\"   ğŸ’¡ ê·¸ì™¸ë¥¼ 1ë“±ê¸‰ìœ¼ë¡œ ì˜¤íŒí•˜ëŠ” ì¹˜ëª…ì  ì˜¤ë¥˜ {fn-fn2}ê°œ ê°ì†Œ!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Recall {target_recall*100}% ë‹¬ì„± ë¶ˆê°€ (ìµœëŒ€ Recall: {recalls.max():.2%})\")\n",
    "\n",
    "# ============================================\n",
    "# 11. ë³€ìˆ˜ ì¤‘ìš”ë„ í™•ì¸\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ë³€ìˆ˜ ì¤‘ìš”ë„ Top 10\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_model = best_model.named_steps['model']\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "n_features_used = len(final_model.feature_importances_)\n",
    "feature_names = feature_names[:n_features_used]\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# 12. êµì°¨ê²€ì¦ vs í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¹„êµ\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ: êµì°¨ê²€ì¦ vs í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# âœ… pos_label=1 ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "test_metrics = {\n",
    "    'PR-AUC': average_precision_score(y_test, y_pred_proba_minority),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F2': fbeta_score(y_test, y_pred, beta=2),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'ROC-AUC': test_roc_auc\n",
    "}\n",
    "\n",
    "print(f\"{'ì§€í‘œ':<15} {'êµì°¨ê²€ì¦ (CV)':<20} {'í…ŒìŠ¤íŠ¸':<15} {'ì°¨ì´':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['PR-AUC', 'Recall', 'F2', 'Precision', 'F1', 'ROC-AUC']:\n",
    "    cv_key = metric.lower().replace('-', '_')\n",
    "    cv_score = results_df.loc[results_df[f'rank_test_{main_score}'] == 1, f'mean_test_{cv_key}'].values[0]\n",
    "    test_score = test_metrics[metric]\n",
    "    diff = test_score - cv_score\n",
    "    \n",
    "    diff_str = f\"{diff:+.4f}\"\n",
    "    if abs(diff) > 0.05:\n",
    "        diff_str += \" âš ï¸\"\n",
    "    \n",
    "    print(f\"{metric:<15} {cv_score:<20.4f} {test_score:<15.4f} {diff_str}\")\n",
    "\n",
    "# ============================================\n",
    "# 13. ìµœì¢… ìš”ì•½\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“ ìµœì¢… ìš”ì•½ ({main_score.upper()} ìµœì í™”)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… ìµœì  ëª¨ë¸ {main_score.upper()} (CV): {lgbm_random.best_score_:.4f}\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ Recall: {test_metrics['Recall']*100:.1f}% (ê·¸ì™¸ ê²€ì¶œë¥ )\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ Precision: {test_metrics['Precision']*100:.1f}% (ê·¸ì™¸ íŒì • ì •í™•ë„)\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ F1: {test_metrics['F1']:.4f}\")\n",
    "print(f\"âœ… ì¹˜ëª…ì  ì˜¤ë¥˜(ë¯¸íƒ FN): {fn}ê°œ / {tp+fn}ê°œ\")\n",
    "print(f\"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {actual_time/60:.1f}ë¶„\")\n",
    "\n",
    "print(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\n",
    "print(f\"   - ê·¸ì™¸(1) {tp+fn}ê°œ ì¤‘ {tp}ê°œ ê²€ì¶œ ({tp/(tp+fn)*100:.1f}%)\")\n",
    "print(f\"   - ë†“ì¹œ ê·¸ì™¸: {fn}ê°œ\")\n",
    "print(f\"   - ì˜¤íƒ (1ë“±ê¸‰â†’ê·¸ì™¸): {fp}ê°œ (ì¬ê²€ì‚¬ í•„ìš”)\")\n",
    "\n",
    "# ============================================\n",
    "# 14. ëª¨ë¸ + íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ì €ì¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "save_model_input = input(\"ëª¨ë¸ì„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \")\n",
    "if save_model_input.lower() == 'y':\n",
    "    \n",
    "    # MODEL_DIRì´ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
    "    try:\n",
    "        MODEL_DIR = Path(MODEL_DIR)\n",
    "    except:\n",
    "        MODEL_DIR = Path('.')\n",
    "        print(f\"âš ï¸  MODEL_DIR ë¯¸ì •ì˜, í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {MODEL_DIR.absolute()}\")\n",
    "    \n",
    "    MODEL_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    model_filename = MODEL_DIR / f'best_lgbm_{main_score}_optimized_{timestamp}.pkl'\n",
    "    params_filename = MODEL_DIR / f'best_params_{main_score}_{timestamp}.json'\n",
    "    metrics_filename = MODEL_DIR / f'test_metrics_{main_score}_{timestamp}.json'\n",
    "    results_filename = MODEL_DIR / f'cv_results_{main_score}_{timestamp}.csv'\n",
    "    \n",
    "    # 1. ëª¨ë¸ ì €ì¥\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"âœ… ëª¨ë¸ ì €ì¥: {model_filename}\")\n",
    "    \n",
    "    # 2. íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "    best_params_serializable = {}\n",
    "    for key, value in lgbm_random.best_params_.items():\n",
    "        if isinstance(value, (np.integer, np.floating)):\n",
    "            best_params_serializable[key] = float(value) if isinstance(value, np.floating) else int(value)\n",
    "        else:\n",
    "            best_params_serializable[key] = value\n",
    "    \n",
    "    params_info = {\n",
    "        'best_params': best_params_serializable,\n",
    "        f'best_{main_score}_score': float(lgbm_random.best_score_),\n",
    "        'optimization_target': main_score,\n",
    "        'cv_folds': n_folds,\n",
    "        'n_iter': n_iter,\n",
    "        'timestamp': timestamp,\n",
    "        'class_labels': {\n",
    "            '0': '1ë“±ê¸‰ (ë‹¤ìˆ˜)',\n",
    "            '1': 'ê·¸ì™¸ (ì†Œìˆ˜)'\n",
    "        },\n",
    "        'minority_count': int(minority_count),\n",
    "        'majority_count': int(majority_count),\n",
    "        'scale_pos_weight': float(scale_pos_weight)\n",
    "    }\n",
    "    \n",
    "    with open(params_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(params_info, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… íŒŒë¼ë¯¸í„° ì €ì¥: {params_filename}\")\n",
    "    \n",
    "    # 3. í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì €ì¥\n",
    "    test_metrics_serializable = {\n",
    "        key: float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        for key, value in test_metrics.items()\n",
    "    }\n",
    "    \n",
    "    metrics_info = {\n",
    "        'test_metrics': test_metrics_serializable,\n",
    "        'confusion_matrix': {\n",
    "            'tn': int(tn),  # 1ë“±ê¸‰ â†’ 1ë“±ê¸‰ (ì •ë‹µ)\n",
    "            'fp': int(fp),  # 1ë“±ê¸‰ â†’ ê·¸ì™¸ (ì˜¤íƒ)\n",
    "            'fn': int(fn),  # ê·¸ì™¸ â†’ 1ë“±ê¸‰ (ë¯¸íƒ, ì¹˜ëª…ì !)\n",
    "            'tp': int(tp)   # ê·¸ì™¸ â†’ ê·¸ì™¸ (ì •ë‹µ)\n",
    "        },\n",
    "        'business_interpretation': {\n",
    "            'total_minority': int(tp + fn),      # ì‹¤ì œ ê·¸ì™¸ ê°œìˆ˜\n",
    "            'detected_minority': int(tp),        # ê²€ì¶œëœ ê·¸ì™¸\n",
    "            'missed_minority': int(fn),          # ë†“ì¹œ ê·¸ì™¸ (ì¹˜ëª…ì )\n",
    "            'recall_percentage': float(tp/(tp+fn)*100) if (tp+fn) > 0 else 0,\n",
    "            'false_alarms': int(fp)              # 1ë“±ê¸‰ì„ ê·¸ì™¸ë¡œ ì˜¤íŒ\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    with open(metrics_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_info, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì €ì¥: {metrics_filename}\")\n",
    "    \n",
    "    # 4. CV ê²°ê³¼ ì €ì¥\n",
    "    results_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "    print(f\"âœ… CV ê²°ê³¼ ì €ì¥: {results_filename}\")\n",
    "    \n",
    "    print(\"\\nğŸ“¦ ì €ì¥ëœ íŒŒì¼ ìš”ì•½:\")\n",
    "    print(f\"1. ëª¨ë¸: {model_filename.name}\")\n",
    "    print(f\"2. íŒŒë¼ë¯¸í„°: {params_filename.name}\")\n",
    "    print(f\"3. ì„±ëŠ¥: {metrics_filename.name}\")\n",
    "    print(f\"4. CVê²°ê³¼: {results_filename.name}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ëª¨ë¸ ì €ì¥ ìƒëµ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClassificationResult:\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    roc_auc: float\n",
    "    pr_auc: float\n",
    "    report: str\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "    \n",
    "\n",
    "class BinaryClassifierTester:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "    \n",
    "    def execute(self, model: Optional[BaseEstimator], name: Optional[str], ext: Optional[str], X_test: pd.DataFrame, y_test: pd.DataFrame, average: str = 'binary'):\n",
    "        pass\n",
    "    \n",
    "    def load_model(self, name: str, ext: str = 'pkl') -> BaseEstimator:\n",
    "        loaded_model = joblib.load(self.root / f'{name}.{ext}')\n",
    "        return loaded_model\n",
    "    \n",
    "    def test(self, model: BaseEstimator, X_test: pd.DataFrame, y_test: pd.DataFrame, average: str = 'binary'):\n",
    "        # í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_prob, average=average)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, digits=3)\n",
    "        bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        pr_auc = average_precision_score(y_test, y_prob)\n",
    "        \n",
    "        return ClassificationResult(\n",
    "            accuracy=bal_acc,\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            f1 = f1,\n",
    "            roc_auc=roc_auc,\n",
    "            pr_auc=pr_auc,\n",
    "            report=report\n",
    "        )\n",
    "        \n",
    "    def show_result(self, result: ClassificationResult):\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(result.report)\n",
    "        print(f\"\\nAccuracy (Test Set): {result.accuracy:.3f}\")\n",
    "        print(f\"ROC-AUC (Test Set): {result.roc_auc:.3f}\")\n",
    "        print(f\"PR-AUC (Test Set): {result.pr_auc:.3f}\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ceb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'best_lgbm_pr_auc_optimized_20251107_125534',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caead454",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(MODEL_DIR / 'best_lgbm_pr_auc_optimized_20251107_125534.pkl')\n",
    "predictions = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0836f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = loaded_model.named_steps['model'].get_params()\n",
    "\n",
    "params['n_estimators'] = 10000\n",
    "params['num_leaves'] = 10\n",
    "params['max_depth'] = 5\n",
    "params['verbosity'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df76826",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "# pipeline = ImbPipeline([\n",
    "#     ('model', LGBMClassifier(**params))\n",
    "# ])\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    **params\n",
    ")\n",
    "\n",
    "lgbm_model.fit(\n",
    "    X_train2, y_train2,\n",
    "    eval_set=[(X_valid, y_valid)],  # train, valid ë‘˜ ë‹¤\n",
    "    eval_names=['train', 'valid'],  # ì´ë¦„ ì§€ì •\n",
    "    eval_metric=['auc_pr', 'recall'],  # PR-AUC (ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©)\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=10000, verbose=True),\n",
    "        log_evaluation(period=100)  # 100 iterationë§ˆë‹¤ ì¶œë ¥\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Recall í‰ê°€ í•¨ìˆ˜\n",
    "# ============================================\n",
    "def recall_eval(y_pred, dtrain):\n",
    "    \"\"\"\n",
    "    Native APIìš© Recall í‰ê°€ í•¨ìˆ˜\n",
    "    \n",
    "    Parameters:\n",
    "    - y_pred: ì˜ˆì¸¡ í™•ë¥  (0~1)\n",
    "    - dtrain: LightGBM Dataset\n",
    "    \n",
    "    Returns:\n",
    "    - metric_name, score, is_higher_better\n",
    "    \"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    score = recall_score(y_true, y_pred_binary)\n",
    "    return 'recall', score, True\n",
    "\n",
    "def pr_auc_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    score = average_precision_score(y_true, y_pred)\n",
    "    return 'pr_auc', score, True\n",
    "\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    score = f1_score(y_true, y_pred_binary)\n",
    "    return 'f1_score', score, True\n",
    "\n",
    "# ============================================\n",
    "# 2. Dataset ìƒì„±\n",
    "# ============================================\n",
    "train_data = lgb.Dataset(X_train2, label=y_train2)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "# ============================================\n",
    "# 3. íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# ============================================\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'None',  # ğŸ”¥ ê¸°ë³¸ ë©”íŠ¸ë¦­ ë„ê¸°\n",
    "    'learning_rate': 0.04330431113219459,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 72,\n",
    "    'min_child_samples': 36,\n",
    "    'min_child_weight': 0.0001,\n",
    "    'reg_alpha': 0.28565947095978705,\n",
    "    'reg_lambda': 0.3310560050532823,\n",
    "    'subsample': 0.8175115631989597,\n",
    "    'colsample_bytree': 0.63045234379612,\n",
    "    'scale_pos_weight': 7.16811515480717,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 1,\n",
    "    'force_col_wise': True,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# 4. í•™ìŠµ (Recall ê¸°ì¤€)\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ LightGBM í•™ìŠµ ì‹œì‘ (Recall ìµœì í™”)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    feval=f1_eval,  # ğŸ”¥ Recall ê¸°ì¤€\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10000, verbose=True),\n",
    "        lgb.log_evaluation(period=100),\n",
    "        lgb.record_evaluation(evals_result)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 5. ê²°ê³¼ í™•ì¸\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_iter = model.best_iteration\n",
    "print(f\"\\nìµœì  Iteration: {best_iter}\")\n",
    "\n",
    "print(\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥:\")\n",
    "# train_recall = evals_result['train']['recall'][best_iter-1]\n",
    "# valid_recall = evals_result['valid']['recall'][best_iter-1]\n",
    "\n",
    "# print(f\"Train Recall: {train_recall:.4f}\")\n",
    "# print(f\"Valid Recall: {valid_recall:.4f}\")\n",
    "\n",
    "# if valid_recall >= 0.5:\n",
    "#     print(f\"\\nâœ… Valid Recall >= 0.5 ë‹¬ì„±!\")\n",
    "# else:\n",
    "#     print(f\"\\nâš ï¸  Valid Recall < 0.5 (í˜„ì¬: {valid_recall:.4f})\")\n",
    "\n",
    "# ============================================\n",
    "# 6. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
    "# ============================================\n",
    "if 'X_test' in locals() and 'y_test' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_pred_proba = model.predict(X_test, num_iteration=best_iter)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    print(f\"PR-AUC: {test_pr_auc:.4f}\")\n",
    "    \n",
    "    if test_recall >= 0.5:\n",
    "        print(f\"âœ… Test Recall >= 0.5 ë‹¬ì„±!\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Test Recall < 0.5 (í˜„ì¬: {test_recall:.4f})\")\n",
    "    \n",
    "    print(\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„ (í´ë˜ìŠ¤ 1=ê·¸ì™¸ ê¸°ì¤€):\")\n",
    "    print(f\"   âœ… ì •í™•í•œ ë¶„ë¥˜: {tn + tp}ê°œ ({(tn+tp)/len(y_test)*100:.1f}%)\")\n",
    "    print(f\"   âš ï¸  ì˜¤íƒ (FP): {fp}ê°œ â†’ 1ë“±ê¸‰ì„ ê·¸ì™¸ë¡œ ì˜¤íŒ\")\n",
    "    print(f\"   ğŸš¨ ë¯¸íƒ (FN): {fn}ê°œ â†’ ê·¸ì™¸ë¥¼ 1ë“±ê¸‰ìœ¼ë¡œ ì˜¤íŒ\")\n",
    "    print(f\"   ğŸ“Š Recall: {tp}ê°œ / {tp+fn}ê°œ = {test_recall*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\në¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['1ë“±ê¸‰', 'ê·¸ì™¸']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "y_prob = loaded_model.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thr = precision_recall_curve(y_test, y_prob)\n",
    "ap = average_precision_score(y_test, y_prob)\n",
    "\n",
    "print(thr)\n",
    "\n",
    "pos_rate = y_test.mean()\n",
    "fig_pr = go.Figure()\n",
    "\n",
    "fig_pr.add_trace(go.Scatter(\n",
    "    x=recall, y=precision,\n",
    "    mode='lines', name=f'PR (AP={ap:.3f})',\n",
    "    line=dict(color='magenta', width=3)\n",
    "))\n",
    "\n",
    "fig_pr.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[pos_rate, pos_rate],\n",
    "    mode='lines', name=f'Baseline pos rate={pos_rate:.2f}',\n",
    "    line=dict(dash='dash', color='grey')\n",
    "))\n",
    "\n",
    "fig_pr.update_layout(\n",
    "    title='Precision-Recall Curve',\n",
    "    xaxis_title='Recall',\n",
    "    yaxis_title='Precision',\n",
    "    template='plotly_dark',\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    xaxis=dict(range=[0, 1]),\n",
    ")\n",
    "\n",
    "fig_pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647723e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "y_prob = loaded_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Balanced Accuracy\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBalanced Accuracy (Test Set): {bal_acc:.3f}\")\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "pr_auc = average_precision_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC (Test Set): {roc_auc:.3f}\")\n",
    "print(f\"PR-AUC (Test Set): {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loaded_model.named_steps['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73915e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP ì ìš©\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute shap value per feature\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "print(shap_importance)\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'shap_importance': shap_importance\n",
    "}).sort_values(by='shap_importance', ascending=False)\n",
    "\n",
    "print(shap_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartfarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
