{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28021e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ \n",
    "\n",
    "# 2-1. ì‹œê°í™”\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "# 2-2. \n",
    "import shap\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, make_scorer, recall_score, \n",
    "    precision_score, f1_score, fbeta_score, average_precision_score, balanced_accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "from scipy.stats import uniform, randint\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils import DATA_DIR, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e26762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "if platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "elif platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "else:  # Linux\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# í°íŠ¸ ê°œì¸ ê²½ë¡œì— ë§ì¶°ì„œ ë³€ê²½\n",
    "# FONT_DIR = Path(\"/path/to/fonts\")\n",
    "# font_path = FONT_DIR / 'FREESENTATION-6SEMIBOLD.ttf'\n",
    "# prop = fm.FontProperties(fname=font_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced663af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_kwargs = {\n",
    "    'parse_dates': ['ê²€ì •ì¼ì'],\n",
    "    'date_format': '%Y-%m-%d'\n",
    "}\n",
    "\n",
    "milk: pd.DataFrame = pd.read_csv(DATA_DIR /'interim' / 'milk.csv', **pandas_kwargs)\n",
    "milk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = milk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # 'ì „ì‚°ì°¨ê±´ìœ ì „ìœ ëŸ‰', \n",
    "    'ì‚°ì°¨', \n",
    "    'ë†í›„ì‚¬ë£Œê¸‰ì—¬ëŸ‰(ì—°ê³„)', \n",
    "    # 'ê³µíƒœì¼ìˆ˜', \n",
    "    'ê³µíƒœì¼ìˆ˜_log', \n",
    "    'ê³„ì ˆ', \n",
    "    'ë†ì¥êµ¬ë¶„', \n",
    "    '305ì¼ìœ ëŸ‰',\n",
    "    # 'ë¶„ë§Œê°„ê²©',\n",
    "    # 'ë¶„ë§Œê°„ê²©_log',\n",
    "    'ì •ì•¡ì½”ë“œë¶„ë¥˜'\n",
    "]\n",
    "\n",
    "target = 'ê°€ê²©ë¯¸ë‹¬'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2824ebb",
   "metadata": {},
   "source": [
    "## ë²”ì£¼í˜• ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e745b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'ì •ì•¡ì½”ë“œë¶„ë¥˜', \n",
    "    # 'ë†ì¥êµ¬ë¶„', \n",
    "    # 'ê³„ì ˆ'\n",
    "]\n",
    "\n",
    "for categorical_feature in categorical_features:\n",
    "    df[categorical_feature] = df[categorical_feature].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9efc8",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2021-08-01'\n",
    "train = df[df['ê²€ì •ì¼ì'] < split_date]\n",
    "test = df[df['ê²€ì •ì¼ì'] >= split_date]\n",
    "\n",
    "X_train, X_test = train[features], test[features]\n",
    "y_train, y_test = train[target], test[target]\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pkg_resources\")\n",
    "\n",
    "# ============================================\n",
    "# 1. í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(\"í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\në¹„ìœ¨:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "\n",
    "# âœ… ìˆ˜ì •: 0ì´ ì¼ë°˜ê°€(ë‹¤ìˆ˜), 1ì´ ê°€ê²©ë¯¸ë‹¬(ì†Œìˆ˜)\n",
    "majority_count = (y_train == 0).sum()   # ì¼ë°˜ê°€ (ë‹¤ìˆ˜) - ì•½ 90%\n",
    "minority_count = (y_train == 1).sum()   # ê°€ê²©ë¯¸ë‹¬ (ì†Œìˆ˜) - ì•½ 10%\n",
    "\n",
    "print(f\"\\ní´ë˜ìŠ¤ êµ¬ì„±:\")\n",
    "print(f\"   0 (ì¼ë°˜ê°€, ë‹¤ìˆ˜): {majority_count}ê°œ ({majority_count/len(y_train)*100:.1f}%)\")\n",
    "print(f\"   1 (ê°€ê²©ë¯¸ë‹¬, ì†Œìˆ˜): {minority_count}ê°œ ({minority_count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# âœ… scale_pos_weight: ì–‘ì„± í´ë˜ìŠ¤(1=ê°€ê²©ë¯¸ë‹¬=ì†Œìˆ˜)ì— ëŒ€í•œ ê°€ì¤‘ì¹˜\n",
    "# ê³µì‹: (ë‹¤ìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜) / (ì†Œìˆ˜ í´ë˜ìŠ¤ ê°œìˆ˜)\n",
    "scale_pos_weight = majority_count / minority_count\n",
    "\n",
    "print(f\"\\nê³„ì‚°ëœ scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "print(f\"   â†’ LightGBMì€ ìë™ìœ¼ë¡œ í´ë˜ìŠ¤ 1(ê°€ê²©ë¯¸ë‹¬)ì— ì´ ê°€ì¤‘ì¹˜ ì ìš©\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "print(\"\\nê²°ì¸¡ì¹˜ í˜„í™©:\")\n",
    "missing_info = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'ê²°ì¸¡_ê°œìˆ˜': X_train.isnull().sum().values,\n",
    "    'ê²°ì¸¡_ë¹„ìœ¨(%)': (X_train.isnull().mean() * 100).values\n",
    "}).sort_values('ê²°ì¸¡_ë¹„ìœ¨(%)', ascending=False)\n",
    "print(missing_info.head(10))\n",
    "print(f\"\\nì „ì²´ ê²°ì¸¡ ë¹„ìœ¨: {X_train.isnull().mean().mean()*100:.1f}%\")\n",
    "print(f\"Feature ê°œìˆ˜: {X_train.shape[1]}ê°œ\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Pipeline êµ¬ì„±\n",
    "# ============================================\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    # ('oversample', RandomOverSampler(sampling_strategy=0.5, random_state=42)),\n",
    "    ('model', LGBMClassifier(\n",
    "        objective='binary',\n",
    "        device='cpu',\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1,\n",
    "        boost_from_average=False,\n",
    "        # scale_pos_weight=scale_pos_weight,  # âœ… ìë™ìœ¼ë¡œ pos_label=1ì— ì ìš©\n",
    "        force_col_wise=True,\n",
    "        metric='None',  # ğŸ”¥ ì¶”ê°€: ê¸°ë³¸ ë©”íŠ¸ë¦­ ë„ê¸°\n",
    "        # ë²”ì£¼í˜• ê´€ë ¨ íŒŒë¼ë¯¸í„°\n",
    "        cat_smooth=10,\n",
    "        cat_l2=10,\n",
    "        max_cat_threshold=32,\n",
    "        max_cat_to_onehot=4\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\nâœ… Pipeline êµ¬ì„±:\")\n",
    "print(\"   1. LGBMClassifier (GPU)\")\n",
    "print(f\"      - scale_pos_weight={scale_pos_weight:.2f} (í´ë˜ìŠ¤ 1=ê°€ê²©ë¯¸ë‹¬ ê°€ì¤‘ì¹˜)\")\n",
    "\n",
    "# ============================================\n",
    "# 3. íŒŒë¼ë¯¸í„° ë¶„í¬\n",
    "# ============================================\n",
    "print(f\"\\nì†Œìˆ˜ í´ë˜ìŠ¤(1=ê°€ê²©ë¯¸ë‹¬) ìƒ˜í”Œ ìˆ˜: {minority_count}ê°œ\")\n",
    "\n",
    "param_distributions = {\n",
    "    # ğŸ”¥ LightGBM íŒŒë¼ë¯¸í„° - ì†Œìˆ˜ í´ë˜ìŠ¤ ë¯¼ê°ë„ í–¥ìƒ\n",
    "    'model__n_estimators': randint(600, 10000),\n",
    "    'model__learning_rate': uniform(0.005, 0.045),\n",
    "    'model__max_depth': randint(3, 8),\n",
    "    'model__num_leaves': randint(15, 80),\n",
    "    'model__min_child_samples': randint(5, 50),\n",
    "    'model__subsample': uniform(0.6, 0.4),\n",
    "    'model__colsample_bytree': uniform(0.6, 0.4),\n",
    "    'model__min_child_weight': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'model__reg_alpha': uniform(0, 0.5),\n",
    "    'model__reg_lambda': uniform(0, 0.5),\n",
    "    \n",
    "    # ë¶ˆê· í˜• ì²˜ë¦¬\n",
    "    'model__scale_pos_weight': uniform(scale_pos_weight, 0.2),\n",
    "    \n",
    "    # ë²”ì£¼í˜• ì²˜ë¦¬\n",
    "    # 'model__cat_l2': uniform(5.0, 15.0), \n",
    "}\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Scoring ì„¤ì •\n",
    "# ============================================\n",
    "\n",
    "scoring = {\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f2': make_scorer(fbeta_score, beta=2),              \n",
    "    'pr_auc': make_scorer(average_precision_score, response_method='predict_proba'),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0, average='macro'),\n",
    "    'roc_auc': 'roc_auc'                                           \n",
    "}\n",
    "\n",
    "main_score = 'precision'\n",
    "\n",
    "print(\"\\nâœ… Scoring ì „ëµ:\")\n",
    "print(f\"   ë©”ì¸ ëª©í‘œ: {main_score.upper()}\")\n",
    "print(f\"   íƒ€ê¹ƒ: í´ë˜ìŠ¤ 1 (ê°€ê²©ë¯¸ë‹¬) ê²€ì¶œ ìµœì í™”\")\n",
    "print(f\"   pos_label: 1 (ê¸°ë³¸ê°’, ìƒëµ ê°€ëŠ¥)\")\n",
    "\n",
    "# ============================================\n",
    "# 5. Stratified K-Fold\n",
    "# ============================================\n",
    "n_folds = 5\n",
    "n_iter = 150\n",
    "cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"\\nêµì°¨ê²€ì¦ Folds: {n_folds}\")\n",
    "print(f\"ëœë¤ ìƒ˜í”Œë§ ì¡°í•© ìˆ˜: {n_iter}ê°œ\")\n",
    "print(f\"ì´ Fits: {n_iter} Ã— {n_folds} = {n_iter * n_folds}\")\n",
    "\n",
    "# ============================================\n",
    "# 6. RandomizedSearchCV ì‹¤í–‰\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ RandomizedSearchCV ì‹œì‘!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lgbm_random = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iter,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    refit=main_score,\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    error_score=np.nan \n",
    ")\n",
    "\n",
    "start_actual = time.time()\n",
    "lgbm_random.fit(X_train, y_train)\n",
    "actual_time = time.time() - start_actual\n",
    "\n",
    "# ============================================\n",
    "# 7. ê²°ê³¼ ì¶œë ¥\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"â±ï¸  ì†Œìš” ì‹œê°„: {actual_time/60:.1f}ë¶„ ({actual_time:.0f}ì´ˆ)\")\n",
    "\n",
    "print(f\"\\nğŸ† ìµœê³  {main_score.upper()} (CV): {lgbm_random.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ ìµœì  íŒŒë¼ë¯¸í„°:\")\n",
    "for key, value in sorted(lgbm_random.best_params_.items()):\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# 8. ì „ì²´ ê²°ê³¼ ë¹„êµ\n",
    "# ============================================\n",
    "results_df = pd.DataFrame(lgbm_random.cv_results_)\n",
    "\n",
    "scoring_cols = [f'mean_test_{score}' for score in scoring.keys()]\n",
    "\n",
    "comparison_cols = [f'rank_test_{main_score}'] + scoring_cols + ['params']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“Š ìƒìœ„ 10ê°œ íŒŒë¼ë¯¸í„° ì¡°í•© ({main_score.upper()} ê¸°ì¤€)\")\n",
    "print(\"=\"*60)\n",
    "top_10 = results_df[comparison_cols].sort_values(f'rank_test_{main_score}').head(10)\n",
    "\n",
    "for idx, row in top_10.iterrows():\n",
    "    rank = int(row[f'rank_test_{main_score}'])\n",
    "    pr_auc = row[f'mean_test_pr_auc']\n",
    "    recall = row['mean_test_recall']\n",
    "    f2 = row['mean_test_f2']\n",
    "    precision = row['mean_test_precision']\n",
    "    f1 = row['mean_test_f1']\n",
    "    \n",
    "    print(f\"\\n#{rank} - {main_score.upper()}: {row[f'mean_test_{main_score}']:.4f} | Recall: {recall:.4f}\")\n",
    "    print(f\"   F2: {f2:.3f} | Precision: {precision:.3f} | F1: {f1:.3f}\")\n",
    "    \n",
    "    params = row['params']\n",
    "    print(f\"   Model: lr={params.get('model__learning_rate', 0):.4f}, \"\n",
    "          f\"depth={params.get('model__max_depth', 0)}, \"\n",
    "          f\"leaves={params.get('model__num_leaves', 0)}\")\n",
    "\n",
    "# ============================================\n",
    "# 9. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = lgbm_random.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# âœ… í´ë˜ìŠ¤ 1(ê°€ê²©ë¯¸ë‹¬)ì˜ í™•ë¥  ì¶”ì¶œ\n",
    "y_pred_proba_minority = y_pred_proba[:, 1]  # âœ… ì¸ë±ìŠ¤ 1 = í´ë˜ìŠ¤ 1 = ê°€ê²©ë¯¸ë‹¬\n",
    "\n",
    "print(classification_report(\n",
    "    y_test, y_pred, \n",
    "    target_names=['0 (ì¼ë°˜ê°€, ë‹¤ìˆ˜)', '1 (ê°€ê²©ë¯¸ë‹¬, ì†Œìˆ˜)']\n",
    "))\n",
    "\n",
    "print(\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['ì‹¤ì œ_0 (ì¼ë°˜ê°€)', 'ì‹¤ì œ_1 (ê°€ê²©ë¯¸ë‹¬)'],\n",
    "                     columns=['ì˜ˆì¸¡_0 (ì¼ë°˜ê°€)', 'ì˜ˆì¸¡_1 (ê°€ê²©ë¯¸ë‹¬)'])\n",
    "print(cm_df)\n",
    "\n",
    "# âœ… í˜¼ë™ í–‰ë ¬ í•´ì„ ìˆ˜ì •\n",
    "# confusion_matrix êµ¬ì¡°:\n",
    "#              ì˜ˆì¸¡_0  ì˜ˆì¸¡_1\n",
    "# ì‹¤ì œ_0 (ì¼ë°˜ê°€)  TN      FP\n",
    "# ì‹¤ì œ_1 (ê°€ê²©ë¯¸ë‹¬)   FN      TP\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„ (ì†Œìˆ˜ í´ë˜ìŠ¤=1=ê°€ê²©ë¯¸ë‹¬ ê¸°ì¤€):\")\n",
    "print(f\"   âœ… ì •í™•í•œ ë¶„ë¥˜: {tn + tp}ê°œ ({(tn+tp)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"   âš ï¸  ì˜¤íƒ (FP): {fp}ê°œ â†’ ì¼ë°˜ê°€ì„ ê°€ê²©ë¯¸ë‹¬ë¡œ ì˜¤íŒ (ì¬ê²€ì‚¬ í•„ìš”)\")\n",
    "print(f\"   ğŸš¨ ë¯¸íƒ (FN): {fn}ê°œ â†’ ê°€ê²©ë¯¸ë‹¬ë¥¼ ì¼ë°˜ê°€ìœ¼ë¡œ ì˜¤íŒ (ì¹˜ëª…ì !)\")\n",
    "\n",
    "# âœ… Recall ê³„ì‚°: TP / (TP + FN)\n",
    "if tp + fn > 0:\n",
    "    recall_pct = tp/(tp+fn)*100\n",
    "    print(f\"\\n   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê°€ê²©ë¯¸ë‹¬) Recall: {recall_pct:.1f}%\")\n",
    "    print(f\"      â†’ ê°€ê²©ë¯¸ë‹¬ {tp+fn}ê°œ ì¤‘ {tp}ê°œ ê²€ì¶œ ({fn}ê°œ ë†“ì¹¨)\")\n",
    "else:\n",
    "    print(f\"\\n   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê°€ê²©ë¯¸ë‹¬) Recall: N/A\")\n",
    "\n",
    "# âœ… Precision ê³„ì‚°: TP / (TP + FP)\n",
    "if tp + fp > 0:\n",
    "    precision_pct = tp/(tp+fp)*100\n",
    "    print(f\"   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê°€ê²©ë¯¸ë‹¬) Precision: {precision_pct:.1f}%\")\n",
    "    print(f\"      â†’ ê°€ê²©ë¯¸ë‹¬ íŒì • {tp+fp}ê°œ ì¤‘ {tp}ê°œ ì •ë‹µ\")\n",
    "else:\n",
    "    print(f\"   ğŸ“Š ì†Œìˆ˜ í´ë˜ìŠ¤(1=ê°€ê²©ë¯¸ë‹¬) Precision: N/A\")\n",
    "\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba_minority)\n",
    "print(f\"   ğŸ“Š ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 10. ğŸ”¥ ì„ê³„ê°’ ì¡°ì •ìœ¼ë¡œ ì¶”ê°€ ê°œì„ \n",
    "# ============================================\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# âœ… pos_label=1 (ê°€ê²©ë¯¸ë‹¬) ê¸°ì¤€ìœ¼ë¡œ PR ê³¡ì„  ê³„ì‚°\n",
    "precisions, recalls, thresholds = precision_recall_curve(\n",
    "    y_test, y_pred_proba_minority\n",
    ")\n",
    "\n",
    "# Recall 80% ë³´ì¥\n",
    "target_recall = 0.80\n",
    "idx = np.where(recalls >= target_recall)[0]\n",
    "\n",
    "if len(idx) > 0:\n",
    "    best_idx = idx[np.argmax(precisions[idx])]\n",
    "    optimal_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ¯ Recall {target_recall*100}% ë³´ì¥ ì‹œ ìµœì  ì„ê³„ê°’\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f} (ê¸°ë³¸ê°’ 0.5)\")\n",
    "    print(f\"Precision: {precisions[best_idx]:.3f}\")\n",
    "    print(f\"Recall: {recalls[best_idx]:.3f}\")\n",
    "    \n",
    "    # âœ… ì„ê³„ê°’ ì¡°ì •: í™•ë¥  >= thresholdì´ë©´ í´ë˜ìŠ¤ 1(ê°€ê²©ë¯¸ë‹¬)ë¡œ ì˜ˆì¸¡\n",
    "    y_pred_adjusted = (y_pred_proba_minority >= optimal_threshold).astype(int)\n",
    "    \n",
    "    cm_adj = confusion_matrix(y_test, y_pred_adjusted)\n",
    "    tn2, fp2, fn2, tp2 = cm_adj.ravel()\n",
    "    \n",
    "    print(f\"\\nì¡°ì • íš¨ê³¼:\")\n",
    "    print(f\"   ë¯¸íƒ(FN) ê°ì†Œ: {fn} â†’ {fn2} (ê°œì„ : {fn-fn2}ê°œ)\")\n",
    "    print(f\"   ì˜¤íƒ(FP) ì¦ê°€: {fp} â†’ {fp2} (ì¦ê°€: {fp2-fp}ê°œ)\")\n",
    "    \n",
    "    if tp + fn > 0 and tp2 + fn2 > 0:\n",
    "        print(f\"   Recall ê°œì„ : {tp/(tp+fn)*100:.1f}% â†’ {tp2/(tp2+fn2)*100:.1f}%\")\n",
    "    \n",
    "    if fn > fn2:\n",
    "        print(f\"   ğŸ’¡ ê°€ê²©ë¯¸ë‹¬ë¥¼ ì¼ë°˜ê°€ìœ¼ë¡œ ì˜¤íŒí•˜ëŠ” ì¹˜ëª…ì  ì˜¤ë¥˜ {fn-fn2}ê°œ ê°ì†Œ!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Recall {target_recall*100}% ë‹¬ì„± ë¶ˆê°€ (ìµœëŒ€ Recall: {recalls.max():.2%})\")\n",
    "\n",
    "# ============================================\n",
    "# 11. ë³€ìˆ˜ ì¤‘ìš”ë„ í™•ì¸\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ë³€ìˆ˜ ì¤‘ìš”ë„ Top 10\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_model = best_model.named_steps['model']\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "n_features_used = len(final_model.feature_importances_)\n",
    "feature_names = feature_names[:n_features_used]\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# 12. êµì°¨ê²€ì¦ vs í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¹„êµ\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ: êµì°¨ê²€ì¦ vs í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# âœ… pos_label=1 ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "test_metrics = {\n",
    "    'PR-AUC': average_precision_score(y_test, y_pred_proba_minority),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F2': fbeta_score(y_test, y_pred, beta=2),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'ROC-AUC': test_roc_auc\n",
    "}\n",
    "\n",
    "print(f\"{'ì§€í‘œ':<15} {'êµì°¨ê²€ì¦ (CV)':<20} {'í…ŒìŠ¤íŠ¸':<15} {'ì°¨ì´':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['PR-AUC', 'Recall', 'F2', 'Precision', 'F1', 'ROC-AUC']:\n",
    "    cv_key = metric.lower().replace('-', '_')\n",
    "    cv_score = results_df.loc[results_df[f'rank_test_{main_score}'] == 1, f'mean_test_{cv_key}'].values[0]\n",
    "    test_score = test_metrics[metric]\n",
    "    diff = test_score - cv_score\n",
    "    \n",
    "    diff_str = f\"{diff:+.4f}\"\n",
    "    if abs(diff) > 0.05:\n",
    "        diff_str += \" âš ï¸\"\n",
    "    \n",
    "    print(f\"{metric:<15} {cv_score:<20.4f} {test_score:<15.4f} {diff_str}\")\n",
    "\n",
    "# ============================================\n",
    "# 13. ìµœì¢… ìš”ì•½\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“ ìµœì¢… ìš”ì•½ ({main_score.upper()} ìµœì í™”)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… ìµœì  ëª¨ë¸ {main_score.upper()} (CV): {lgbm_random.best_score_:.4f}\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ Recall: {test_metrics['Recall']*100:.1f}% (ê°€ê²©ë¯¸ë‹¬ ê²€ì¶œë¥ )\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ Precision: {test_metrics['Precision']*100:.1f}% (ê°€ê²©ë¯¸ë‹¬ íŒì • ì •í™•ë„)\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ F1: {test_metrics['F1']:.4f}\")\n",
    "print(f\"âœ… ì¹˜ëª…ì  ì˜¤ë¥˜(ë¯¸íƒ FN): {fn}ê°œ / {tp+fn}ê°œ\")\n",
    "print(f\"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {actual_time/60:.1f}ë¶„\")\n",
    "\n",
    "print(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\n",
    "print(f\"   - ê°€ê²©ë¯¸ë‹¬(1) {tp+fn}ê°œ ì¤‘ {tp}ê°œ ê²€ì¶œ ({tp/(tp+fn)*100:.1f}%)\")\n",
    "print(f\"   - ë†“ì¹œ ê°€ê²©ë¯¸ë‹¬: {fn}ê°œ\")\n",
    "print(f\"   - ì˜¤íƒ (ì¼ë°˜ê°€â†’ê°€ê²©ë¯¸ë‹¬): {fp}ê°œ (ì¬ê²€ì‚¬ í•„ìš”)\")\n",
    "\n",
    "# ============================================\n",
    "# 14. ëª¨ë¸ + íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ì €ì¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "save_model_input = input(\"ëª¨ë¸ì„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \")\n",
    "if save_model_input.lower() == 'y':\n",
    "    \n",
    "    # MODEL_DIRì´ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
    "    try:\n",
    "        MODEL_DIR = Path(MODEL_DIR)\n",
    "    except:\n",
    "        MODEL_DIR = Path('.')\n",
    "        print(f\"âš ï¸  MODEL_DIR ë¯¸ì •ì˜, í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {MODEL_DIR.absolute()}\")\n",
    "    \n",
    "    MODEL_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    model_filename = MODEL_DIR / f'best_lgbm_{main_score}_optimized_{timestamp}.pkl'\n",
    "    params_filename = MODEL_DIR / f'best_params_{main_score}_{timestamp}.json'\n",
    "    metrics_filename = MODEL_DIR / f'test_metrics_{main_score}_{timestamp}.json'\n",
    "    results_filename = MODEL_DIR / f'cv_results_{main_score}_{timestamp}.csv'\n",
    "    \n",
    "    # 1. ëª¨ë¸ ì €ì¥\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"âœ… ëª¨ë¸ ì €ì¥: {model_filename}\")\n",
    "    \n",
    "    # 2. íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "    best_params_serializable = {}\n",
    "    for key, value in lgbm_random.best_params_.items():\n",
    "        if isinstance(value, (np.integer, np.floating)):\n",
    "            best_params_serializable[key] = float(value) if isinstance(value, np.floating) else int(value)\n",
    "        else:\n",
    "            best_params_serializable[key] = value\n",
    "    \n",
    "    params_info = {\n",
    "        'best_params': best_params_serializable,\n",
    "        f'best_{main_score}_score': float(lgbm_random.best_score_),\n",
    "        'optimization_target': main_score,\n",
    "        'cv_folds': n_folds,\n",
    "        'n_iter': n_iter,\n",
    "        'timestamp': timestamp,\n",
    "        'class_labels': {\n",
    "            '0': 'ì¼ë°˜ê°€ (ë‹¤ìˆ˜)',\n",
    "            '1': 'ê°€ê²©ë¯¸ë‹¬ (ì†Œìˆ˜)'\n",
    "        },\n",
    "        'minority_count': int(minority_count),\n",
    "        'majority_count': int(majority_count),\n",
    "        'scale_pos_weight': float(scale_pos_weight),\n",
    "        'features': features,\n",
    "        'train_test_split': split_date\n",
    "    }\n",
    "    \n",
    "    with open(params_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(params_info, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… íŒŒë¼ë¯¸í„° ì €ì¥: {params_filename}\")\n",
    "    \n",
    "    # 3. í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì €ì¥\n",
    "    test_metrics_serializable = {\n",
    "        key: float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        for key, value in test_metrics.items()\n",
    "    }\n",
    "    \n",
    "    metrics_info = {\n",
    "        'test_metrics': test_metrics_serializable,\n",
    "        'confusion_matrix': {\n",
    "            'tn': int(tn),  # ì¼ë°˜ê°€ â†’ ì¼ë°˜ê°€ (ì •ë‹µ)\n",
    "            'fp': int(fp),  # ì¼ë°˜ê°€ â†’ ê°€ê²©ë¯¸ë‹¬ (ì˜¤íƒ)\n",
    "            'fn': int(fn),  # ê°€ê²©ë¯¸ë‹¬ â†’ ì¼ë°˜ê°€ (ë¯¸íƒ, ì¹˜ëª…ì !)\n",
    "            'tp': int(tp)   # ê°€ê²©ë¯¸ë‹¬ â†’ ê°€ê²©ë¯¸ë‹¬ (ì •ë‹µ)\n",
    "        },\n",
    "        'business_interpretation': {\n",
    "            'total_minority': int(tp + fn),      # ì‹¤ì œ ê°€ê²©ë¯¸ë‹¬ ê°œìˆ˜\n",
    "            'detected_minority': int(tp),        # ê²€ì¶œëœ ê°€ê²©ë¯¸ë‹¬\n",
    "            'missed_minority': int(fn),          # ë†“ì¹œ ê°€ê²©ë¯¸ë‹¬ (ì¹˜ëª…ì )\n",
    "            'recall_percentage': float(tp/(tp+fn)*100) if (tp+fn) > 0 else 0,\n",
    "            'false_alarms': int(fp)              # ì¼ë°˜ê°€ì„ ê°€ê²©ë¯¸ë‹¬ë¡œ ì˜¤íŒ\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    with open(metrics_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics_info, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì €ì¥: {metrics_filename}\")\n",
    "    \n",
    "    # 4. CV ê²°ê³¼ ì €ì¥\n",
    "    results_df.to_csv(results_filename, index=False, encoding='utf-8')\n",
    "    print(f\"âœ… CV ê²°ê³¼ ì €ì¥: {results_filename}\")\n",
    "    \n",
    "    print(\"\\nğŸ“¦ ì €ì¥ëœ íŒŒì¼ ìš”ì•½:\")\n",
    "    print(f\"1. ëª¨ë¸: {model_filename.name}\")\n",
    "    print(f\"2. íŒŒë¼ë¯¸í„°: {params_filename.name}\")\n",
    "    print(f\"3. ì„±ëŠ¥: {metrics_filename.name}\")\n",
    "    print(f\"4. CVê²°ê³¼: {results_filename.name}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ëª¨ë¸ ì €ì¥ ìƒëµ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ceb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'best_lgbm_pr_auc_optimized_20251108_215407',\n",
    "    'best_lgbm_pr_auc_optimized_20251108_222518',\n",
    "    'best_lgbm_pr_auc_optimized_20251108_225509',\n",
    "    'best_lgbm_pr_auc_optimized_20251108_233026',\n",
    "    'best_lgbm_pr_auc_optimized_20251108_235658',\n",
    "    'best_lgbm_pr_auc_optimized_20251109_002743',\n",
    "    'best_lgbm_pr_auc_optimized_20251109_025011',\n",
    "    'best_lgbm_pr_auc_optimized_20251109_033530',\n",
    "    'best_lgbm_precision_optimized_20251109_041549'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caead454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "loaded_model = joblib.load(MODEL_DIR / 'best_lgbm_precision_optimized_20251109_041549.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df76826",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f983bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647723e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "y_prob = loaded_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_prob >= 0.576).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Balanced Accuracy\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBalanced Accuracy (Test Set): {bal_acc:.3f}\")\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "pr_auc = average_precision_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC (Test Set): {roc_auc:.3f}\")\n",
    "print(f\"PR-AUC (Test Set): {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f1a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568df66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_list = np.round(np.arange(0.1, 1.0, 0.01), 2)\n",
    "rows = []\n",
    "for thr in thr_list:\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    rows.append([\n",
    "        thr,\n",
    "        precision_score(y_test, y_hat, zero_division=0),\n",
    "        recall_score(y_test, y_hat, zero_division=0),\n",
    "        f1_score(y_test, y_hat, zero_division=0)\n",
    "    ])\n",
    "\n",
    "thr_table = pd.DataFrame(rows, columns=['threshold', 'precision', 'recall', 'f1'])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thr_table[\"threshold\"], y=thr_table[\"precision\"],\n",
    "                         mode=\"lines\", name=\"Precision\",\n",
    "                         line=dict(dash=\"dash\", color=\"cyan\")))\n",
    "fig.add_trace(go.Scatter(x=thr_table[\"threshold\"], y=thr_table[\"recall\"],\n",
    "                         mode=\"lines\", name=\"Recall\",\n",
    "                         line=dict(color=\"magenta\")))\n",
    "fig.add_trace(go.Scatter(x=thr_table[\"threshold\"], y=thr_table[\"f1\"],\n",
    "                         mode=\"lines\", name=\"F1\",\n",
    "                         line=dict(color=\"yellow\")))\n",
    "\n",
    "fig.update_layout(title=\"ì„ê³—ê°’ ë³€í™”ì— ë”°ë¥¸ Precision/Recall/F1\",\n",
    "                  xaxis_title=\"Threshold\",\n",
    "                  yaxis_title=\"Score\",\n",
    "                  yaxis=dict(range=[0,1]),\n",
    "                  template=\"plotly_dark\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# 1) PR Curve ê³„ì‚°\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_prob)\n",
    "ap = average_precision_score(y_test, y_prob)\n",
    "\n",
    "# 2) ì–‘ì„± ë¹„ìœ¨ (baseline)\n",
    "pos_rate = y_test.mean()\n",
    "\n",
    "# 3) Plotly ì‹œê°í™”\n",
    "fig_pr = go.Figure()\n",
    "\n",
    "# PR ê³¡ì„ \n",
    "fig_pr.add_trace(go.Scatter(\n",
    "    x=rec, y=prec,\n",
    "    mode=\"lines\", name=f\"PR (AP={ap:.3f})\",\n",
    "    line=dict(color=\"magenta\", width=3)\n",
    "))\n",
    "\n",
    "# ê¸°ì¤€ì„  (ì–‘ì„± ë¹„ìœ¨)\n",
    "fig_pr.add_trace(go.Scatter(\n",
    "    x=[0,1], y=[pos_rate, pos_rate],\n",
    "    mode=\"lines\", name=f\"Baseline pos rate={pos_rate:.2f}\",\n",
    "    line=dict(dash=\"dash\", color=\"grey\")\n",
    "))\n",
    "\n",
    "fig_pr.update_layout(\n",
    "    title=\"Precision-Recall Curve\",\n",
    "    xaxis_title=\"Recall\",\n",
    "    yaxis_title=\"Precision\",\n",
    "    template=\"plotly_dark\",\n",
    "    yaxis=dict(range=[0,1]),\n",
    "    xaxis=dict(range=[0,1])\n",
    ")\n",
    "\n",
    "fig_pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loaded_model.named_steps['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73915e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# SHAP ì ìš©\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute shap value per feature\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "print(shap_importance)\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'shap_importance': shap_importance\n",
    "}).sort_values(by='shap_importance', ascending=False)\n",
    "\n",
    "print(shap_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartfarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
